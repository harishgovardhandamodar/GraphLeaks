\section{Mitigating Privacy Risks}\label{discuss}

We discuss some potential mitigation strategies to lower the privacy risks from the proposed algorithms and keep the evaluations as future work.

\noindent\textbf{Lower Embedding Precision.} The embeddings are continuous values and capture the semantic information about the graph structure.
These embeddings while reduce the overall dimensionality of the graph data also capture rich information about the input graphs by ensuring that the properties in the graph are still maintained in the embedding.
Lowering the precision of the embedding vector for each node by rounding can help reduce the attack model from learning rich features about the inputs~\cite{membershipinf,nlp}.

\noindent\textbf{Adversarial Examples.} In the proposed attacks, the attacker model typically involves a machine learning algorithm (e.g node and attribute inference, decoder for graph reconstruction).
Adversarial examples are imperceptible noise added to the output prediction to force the target model to misclassify.
Since, the attack models are vulnerable to adversarial examples, the released embedding can be released with an additional adversarial noise to misclassify the target model.
However, the noise added to be optimized to ensure utility for further downstream applications~\cite{attriguard,memguard}.

\noindent\textbf{Adversarial Training.} In case of node membership inference can be modelled as an minimax adversarial training with joint optimization to minimize the model loss using the graph embeddings (e.g GNNs) while maximising the adversary's loss on inferring the sensitive inputs.
Such a joint optimization can be used to update the embeddings such that they maintain the original graph properties while ensuring that the attack model performance is low.
For inference time, such defences have been explored in the context of traditional machine learning to protect against membership inference~\cite{advreg} and preserve privacy of text models~\cite{textembleak}.

\noindent\textbf{Differential Privacy.} One major approach to protect an individual user's privacy is to add noise sampled from Laplacian and Gaussian distribution.
This carefully added noise ensures that the presence or absence of a user's data in the mechanism does not change the model output.
Further, DP provides a theoretical bound on the total privacy leakage from the mechanism (downstream processing from embeddings) on an individual's data point.
Such DP based embeddings have been proposed for both graph and text models~\cite{dptext,dpne}, however, their efficacy against the proposed privacy attacks are yet to be studied as part of the future work.
