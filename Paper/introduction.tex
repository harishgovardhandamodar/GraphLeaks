\section{Introduction}\label{introduction}


Large scale real-world systems are typically modelled in the form of graphs: online social networks, world wide web, citation networks and biomedical datasets, which represent the entities as nodes and their relationship with edges~\cite{zhou2018graph}.
Traditional Deep Neural Networks fail to capture the nuances of structured data but a specific class of algorithms, namely, Graph Neural Networks (GNNs) have shown state of the art performance on such complex graph data for node classification, link prediction etc.
An important pre-processing step for using graph data with machine learning is embedding the high dimensional graph data to a low dimensional representation for easy processing by machine learning algorithms.
In many applications, such embeddings are released for further processing to save storage cost without considering the privacy implications.
Such large graph dataset raises the question of privacy specifically if the algorithms are trained with private and potentially sensitive data.
Consider a graph capturing the outbreak of a disease where the nodes represent the individuals, medical symptoms as the node features and the edges indicating the disease transmission.
Typically, in such datasets a GNN provides state of the art performance for predicting disease for an arbitrary user in the graph (node classification) and determining the future outbreak (link prediction).
For such a model which does not account privacy, an adversary can however infer the health status of a particular user (node in graph) by identifying whether the user was part of the training data or not.
Further, the adversary can potentially reconstruct the graph from the low dimensional embeddings enabling the adversary to extract the sensitive input to the models.
Finally, graph embeddings capture important semantics from the input graph while maintaining the contextual information in the form of preferential connection which can be exploited to infer sensitive attributes about an individual.
These three privacy attacks, namely, membership inference, graph reconstruction and attribute inference, are examples of a direct privacy violation of the individual which can further be used without user consent. % for deciding medical insurance premium or job hiring.
Further, companies spend enormous resources to annotate the training dataset to achieve state of the art performance and such attacks inferring training data also violates the Intellectual Property.
Hence, studying and mitigating privacy risks is crucial specifically with the onset of data protection laws such as HIPAA and GDPR.

In the context of Machine Learning, a privacy violation occurs when an adversary infers something about a \textit{particular} user's data record in the training dataset which cannot be inferred from other models trained on similar data distribution~\cite{membershipinf,whitebox}.
This information leakage is quantified using the success of inference attacks. %: membership inference, attribute inference and reconstruction attacks.
In attribute inference attacks, the attacker infers sensitive features of an individual's data record used in model's training.
A stronger case of attribute inference is where the attacker reconstructs a portion of the sensitive training data itself, i.e, data reconstruction attack.
In case of membership inference, the adversary traces a particular individual's record to the training dataset, i.e., identify whether a given data record was a member of the training data.
Prior literature in privacy attacks focus on models trained on non-graph data including text, images and speech to study the vulnerability to membership inference~\cite{ndss19salem,membershipinf}, attribute inference~\cite{attributeinf,attributeinf2}, property inference~\cite{propertyinf}, model inversion~\cite{modelinversion} attacks as well as model parameter and hyperparameter stealing attacks~\cite{timing,stealml,8418595}.
However, the privacy risk in Graph-based machine learning models under adversarial setting is not fully explored.

%In this work, we evaluate the vulnerability of graph embedding algorithms against the threat from three privacy attacks: membership inference, graph reconstruction and attribute inference.

In this work, we propose the first comprehensive privacy analysis of Graph Embedding algorithms under different threat models and adversary assumptions.
We mainly focus on exploiting publicly released graph embeddings trained with private data, used for different downstream tasks, under various attacks which violates the user's data privacy: membership inference, graph reconstruction and attribute inference.
First, we evaluate the privacy leakage under membership inference attacks though a blackbox and whitebox settings.
The blackbox setting considers the specific case of downstream node classification task for convolution kernel based graph embedding with neural network. In this setting, we propose two attacks for membership inference: shadow model attack and confidence score attack.
Here, we show that the proposed attacks have an inference accuracy of 78\%, 63\%, and 60\% for confidence score attack and 62\%, 60\%, and 55\% for shadow model attack, respectively for three different datasets (i.e., Cora, Citesser and Pubmed dataset.
For the whitebox setting, we propose an unsupervised attack for the more generic case of using just the graph embeddings to differentiate whether a given node was part of the training graph or not.
We show that an adversary in this setting can predict the training data with a high accuracy (70\% on average on the three datasets).

Second, we propose a novel graph reconstruction attack where the adversary, given access to the node embeddings of a subgraph, trains an encoder-decoder model to reconstruct the target graph from its publicly released embeddings.
This attack has serious privacy implications since the adversary reconstructs the input graph dataset which can be potentially sensitive.
The proposed attack has high precision: 0.722 for Cora, 0.778 for Citeseer and 0.95 for Pubmed dataset.
Moreover, on increasing the adversary's prior knowledge, the attack performance increases significantly.
An important privacy implication is link inference, i.e, predicting whether there exists a link between any two nodes in the graph.
Through this attack, an adversary infers a link between nodes with 93\%, 90\% and 57\% of accuray for respectively Cora, Citeseer and Pubmed dataset, compared to the 50\% baseline random guess accuracy.


Finally, we propose the attribute inference attack where the adversary tries to infer sensitive attributes for user node in the graph using the released graph embeddings.
We consider two state of the art unsupervised random walk based embeddings, Node2Vec and DeepWalk, on two real-world social networking datasets: Facebook and LastFM, where the adversary aims to infer the user gender and location, respectively.
Given access to the embeddings of a subgraph and corresponding sensitive attributes, we model attribute inference as a supervised learning problem.
The adversary trains a supervised attack model to predict sensitive hidden attributes for target users given the released publicly available target embeddings.
Here, the attack model's F1 score (capturing the balance between precision and recall) on LastFM was as high as 0.65 for DeepWalk and 0.83 for Node2Vec. 
For Facebook, the F1 score was 0.61 for Node2Vec and 0.59 for DeepWalk.
The paper indicates the serious data privacy risks in graph data processing algorithms and calls for further research to design privacy-preserving embedding algorithms for graph data.

%\noindent\textbf{Paper Organization.}
The paper is organized as follows. 
We introduce the basic idea behind graph embedding algorithms and GNNs in Section~\ref{background}, followed by a detailed attack taxonomy and threat models for the three proposed privacy attacks: Membership Inference, Graph Reconstruction and Attribute Inference attacks in Section~\ref{attack}.
We describe the experimental setup with the baselines and dataset descriptions in Section~\ref{setup}.
Subsequently, the evaluations of the proposed attacks under different threat models and settings are given in Section~\ref{evaluation}.
Finally, we discuss potential mitigation techniques to protect the graph data privacy and conclude in Section~\ref{conclusions}.




















