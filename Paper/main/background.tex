\section{Background}\label{background}

A large number of real-world applications require processing graph data which contains rich relational information between different entities (e.g., online social media, disease outbreaks, recommendation engines, knowledge graphs and navigation systems)~\cite{zhou2018graph}.
Deep Learning and more precisely Convolutional Neural Networks have shown tremendous performance over non-graph data such as images by capturing the spatial relation between pixels of an image and extracting features over multiple layers.
However, this machine learning scheme has shown its limits for graph data and the learning on such data is still challenging~\cite{zhou2018graph}.
Indeed, the models have to capture the connections in the data while ensuring invariance of graph data representation, even without fixed ordering between the nodes (i.e., the adjacency matrix representing the connections between nodes varies but still results in the same graph). %different
To overcome this limitation, the graph data is passed through embedding algorithms that map the large graphs to lower dimensions which are then used for downstream processing with GNNs.
Graph embedding algorithms enable models operating on low dimensional euclidean datasets (i.e., such as images) to graph data by mapping them into a low dimensional embedding.
We represent a graph as $G=(V,E)$ where $V$ represents the vertex set consisting of nodes \{$v_1,...,v_n$\} where the connections between the edges $E$ is represented as a symmetric, sparse adjacency matrix $A$ $\in$ $R_{nxn}$ where $a_{ij}$ denotes the edge weight between nodes with $a_{ij}= 0$ for missing edges.


\subsection{Graph Embedding Algorithms}

To mitigate the space and computation overhead of downstream graph processing, graph embedding algorithms provide an efficient approach to represent the graph data in a low dimension embedding~\cite{tutorial}.
Specifically, an embedding algorithm $\Psi: V \rightarrow \mathcal{R}^d$ where nodes $V$ $\in$ $G$.
The embedding $\Psi(v)$ of a single node $v$ is hence an $d$-dimensional vector capturing the properties of the original graph such as distance from other nodes.
Different graph embedding algorithms to embed both the entire graphs as well as the nodes have been well studied~\cite{survey}.
Random walk based node embedding algorithms traverses the graph to sample random walks which are then passed as sentences to SkipGram algorithm to obtain the corresponding node features~\cite{node2vec,deepwalk}.
In deep learning-based graph embeddings, both the features along with adjacency matrix can be used to generate low dimensional embeddings for each of the nodes.
For generating these embeddings, the parameters of the embedding function are updated to improve the representation of the graph nodes while maintaining the original properties.
These are typically modeled using GNNs and Graph Convolutional Networks.
In this work, we mainly focus on node embeddings which we refer to as graph embeddings throughout the paper.
We consider random walk based embeddings for attribute inference and deep learning-based embeddings for node inference and graph reconstruction attacks.


\subsection{Graph Neural Networks}

The initial layers of a GNN are used to generate embedding for the input graphs which can be extended for node classification and link prediction tasks by attacking a classifier network as GNNs.
The pre-processed graph in the form of embeddings along with the node features is represented as matrices for computation.
The training of GNNs relies on a message-passing algorithm which is the weighted aggregation of features of neighboring nodes $\mathcal{N}(v)$ to compute the feature of a particular node $v$.
Given the features $x$ of a single node $v$, the GNN produces an output label $f(x;W)$ which captures the probability of the input node with features $x$ belonging to a particular class.
The loss over the resultant classification for the node $v$ is then backpropagated to update the model weights for aggregation.
Consider a $N\times D_F$ feature matrix $X$ where $N$ is the number of nodes, $D_F$ is the number of node features and an adjacency matrix $A$ which captures the representation of graph structure in matrix form.
The output of a layer with $F$ features takes the feature matrix along with the adjacency matrix as input to produce a $N\times F$ matrix as an output.
The computation is given by $H^{(l+1)} = f_{agg}(H^{(l)}, A)$ with $H(0)=X$ and $H(L)=Z$, $L$ being the number of layers and $H$ is the intermediate activation.
Based on the different aggregation function $f_{agg}()$, we obtain different GNN algorithms such as Graph Convolutional Network (GCN)~\cite{Kipf2016tc}, GraphSAGE \cite{NIPS20176703}, Graph Attention Networks (GAT)~\cite{velickovic2018graph} and Topology Adaptive GCN (TAGCN)~\cite{du2018topology}.


%In this work, we specifically focus on inductive training of GNN where the model does not see test nodes during training unlike transductive learning where the entire graph and features are available apriori.
%Given the full graph $G_{full}$, we sample a subgraph $G_{train}$ which is used for training the models and evaluate the model performance on the held out graph $G_{full}-G_{train}$.
%Such an inductive setting enables the adversary to learn new information about the target model's training graph resulting in a privacy leakage.
