\section{Related Work}
\label{related}

The wide availability of location and mobility data has been followed by the development of machine learning schemes to predict interests, colocations or other user information~\cite{noulas2009inferring}. 
While traditional approaches define features to characterize usersâ€™ mobility useful for prediction tasks, an important pre-processing step for using graph data with machine learning is embedding the high dimensional graph data to a low dimensional representation for easy processing by machine learning algorithms~\cite{yang2019revisiting}.
In this context, GNNs~\cite{zhou2018graph} have shown state of the art performance on such complex graph data for node classification, link prediction etc.
However, the privacy implications of the use of such embeddings have not been fully considered.

Inference attacks that violate data privacy have been explored in the context of traditional machine learning models.
Membership Inference attacks can be deployed in both whitebox~\cite{whitebox} and blackbox~\cite{membershipinf} setting in traditional machine learning algorithms.
These attacks are further extended to collaborative learning~\cite{collabinf,whitebox} and generative models~\cite{logan}.
On the other hand, reconstruction attacks infer private attributes of the inputs passed to the models~\cite{attributeinf, attributeinf2, propertyinf, modelinversion}.
Other privacy attacks aim to extract hyperparameters~\cite{8418595}, reverse engineer the model architecture and parameters using side channels~\cite{timing} or the output predictions~\cite{stealml}.
Memorization of data by Neural Networks has been attributed as a major cause for privacy leakage~\cite{memorize,secretsharer,overlearninginf}.
%
Further, recent works have indicated privacy risks in Graph NNs where an adversary can infer the presence of a link between two nodes using a manual threshold between the distance of two node features~\cite{linksteal}.
This attack however, is subsumed within our more generic attack methodology where we extract the entire adjacency matrix which can be used to infer the presence of links.
Text models have been shown to leak user data through attribute inference and inversion attacks~\cite{textembleak,nlp}. 
However, a direct application of these attacks is not possible in case of high dimensional graphs and requires additional consideration to the network structure making our problem challenging.
Other than privacy attacks, adversarial attacks against GNNs~\cite{graphatt,nodepoison} have been explored as well as training algorithms to enhance the robustness against such attacks~\cite{robustdef1,robustdef2}.

To mitigate Membership attacks, Memguard~\cite{memguard} and AttriGuard~\cite{attriguard} add carefully crafted noise to the final output prediction to misclassify the shadow model attacks.
Adversarial regularization using minimax optimization regularizes the model to mitigate inference attacks~\cite{advreg}.
Regularization through ensemble training, dropout and L2-regularization have been studied~\cite{ndss19salem}.
Differential Privacy mitigates such privacy attacks with theoretical guarantees by adding noise to gradients but faces an unbalanced privacy accuracy trade-off~\cite{diffpriv}.
Such Differential Privacy frameworks have also been explored in the context of graph and text embeddings~\cite{dptext,dpne} but their efficacy on lowering privacy risks from the proposed attacks is yet to be explored.
