%\documentclass[conference]{IEEEtran}
\documentclass[sigconf]{acmart}


%\settopmatter{printacmref=false} % Removes citation information below abstract
%\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
%\pagestyle{plain} % removes running headers

\usepackage{multirow}
\usepackage{dblfloatfix}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{backgrounds, positioning, fit}
\usetikzlibrary{shapes.geometric}
\usepackage{amsmath}
\usetikzlibrary{patterns}
\usetikzlibrary{pgfplots.groupplots}
\usepackage{subfigure}
\usepackage{float}
\usepackage{wrapfig}


\begin{document}
%\pagestyle{plain}

\title{Quantifying Privacy Leakage in Graph Embedding}



\author{Vasisht Duddu}
\affiliation{%
  \institution{Univ Lyon, INSA Lyon, Inria, CITI}
}
\email{vduddu@tutamail.com}

\author{Antoine Boutet}
\affiliation{%
	\institution{Univ Lyon, INSA Lyon, Inria, CITI}
}
\email{antoine.boutet@insa-lyon.fr}

\author{Virat Shejwalkar}
\affiliation{%
	\institution{University of Massachusetts Amherst}
}
\email{vshejwalkar@cs.umass.edu}

%\author{
%    \IEEEauthorblockN{Vasisht Duddu\IEEEauthorrefmark{1}, Antoine Boutet\IEEEauthorrefmark{1},  Virat Shejwalkar\IEEEauthorrefmark{2}}
%    \IEEEauthorblockA{\IEEEauthorrefmark{1}Univ Lyon, INSA Lyon, Inria, CITI}
%    \IEEEauthorblockA{\IEEEauthorrefmark{2}University of Massachusetts Amherst}
%    \IEEEauthorblockA{vduddu@tutamail.com, antoine.boutet@insa-lyon.fr, vshejwalkar@cs.umass.edu}
%}


\copyrightyear{2020}
\acmYear{2020}
\setcopyright{acmcopyright}
\acmConference[MobiQuitous '20]{EAI International Conference on Mobile and
Ubiquitous Systems: Computing, Networking and Services}{November 7--9, 2020}{N/A, Cyberspace}
\acmBooktitle{EAI International Conference on Mobile and Ubiquitous Systems:
Computing, Networking and Services (MobiQuitous '20), November 7--9, 2020, N/A, Cyberspace}
\acmPrice{ }
\acmDOI{ }
\acmISBN{ }


%\maketitle

\begin{abstract}


Graph embeddings have been proposed to map graph data to low dimensional space for downstream processing (e.g., node classification or link prediction). With the increasing collection of personal data, graph embeddings can be trained on private and sensitive data. For the first time, we quantify the privacy leakage in graph embeddings through three inference attacks targeting Graph Neural Networks. We propose a membership inference attack to infer whether a graph node corresponding to an individual user's data was a member of the model's training or not. We consider a blackbox setting where the adversary exploits the output prediction scores and a whitebox setting where the adversary has also access to the released node embeddings. This attack provides accuracy up to 28\% (blackbox) and 36\% (whitebox) beyond random guess by exploiting the distinguishable footprint between train and test data records left by the graph embedding. We propose a Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings. Here, the adversary can reconstruct the graph with more than 80\% of accuracy and link inference between two nodes with around 30\% more confidence than a random guess. We then propose an attribute inference attack where the adversary aims to infer a sensitive attribute. We show that graph embeddings are strongly correlated to the node attributes letting the adversary inferring sensitive information (e.g., gender or location).


%Graphs are ubiquitous to model the relationship between nodes representing various types of data.
%Consequently, graph embeddings have been proposed to map graph data to low dimensional space for downstream processing such as node classification and link prediction.
%With the increasing collection and processing of personal data, graph embeddings can be trained on potentially private and sensitive data.
%In this work, for the first time, we study and quantify the privacy leakage in graph embeddings by proposing novel inference attacks targeting graph based deep learning models.
%These inference attacks are considered in different settings.
%Firstly, given a blackbox Graph Neural Network for node classification, we propose a membership inference attack to infer whether a graph node corresponding to individual user's data was member of the model's training or not by exploiting the output prediction scores.
%In this blackbox setting, we show that the attack results in privacy leakage of 28\% beyond random guess.
% what is the value for random guess? 50%? it is possible to say the benefice to the adversary differently?
%We then further extend this attack to whitebox setting where the adversary has access to the released node embeddings.
%This attack in whitebox setting exploits the fact graph embedding algorithms leave a distinguishable footprint between train and test data records, providing attack accuracy as high as 36\% beyond random guess.
%Secondly, we propose a Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings.
%In this threat model, we show that the adversary can reconstruct the graph with high precision (more than 80\% of accuracy on average on three datasets).
%A serious implication of reconstruction is link inference, where the adversary can infer the presence of an edge between two nodes in a graph with high accuracy (around 30\% more confidence than a random guess).
%Finally, we propose an attribute inference attack where the adversary aims to infer a sensitive attribute.
%We show that graph embeddings are strongly correlated to node attributes from which the adversary can infer sensitive information such as gender and location.
% do we consider collusion? remains to future work?






%Graphs are ubiquitous to model the relationship between nodes representing various types of data.
%Consequently, graph embedding algorithms have been proposed to map graph data to low dimensional space for downstream processing such as node classification and link prediction.
%With the increasing collection and processing of personal data, graph embeddings can be trained on potentially private and sensitive graph data.
%In this work, we study and quantify the privacy leakage in graph embeddings by proposing novel inference attacks specific to Graph based deep learning models.
%Given a blackbox Graph Neural Network for node classification, we propose membership inference attack to infer whether a graph node corresponding to individual user's data was member of the model's training or not by exploiting the output prediction scores.
%In this blackbox setting, we show that the attack gives the adversary an inference advantage as high as 56\% beyond random guess.
%We further extend the attack to whitebox setting where adversary has access to the released node embeddings to indicate an adversary advantage as high as 72\%.
%These attacks exploit the fact graph embedding algorithms leave a distinguishable footprint between train and test data records.
%We then propose Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings.
%In this threat model, we show the adversary can reconstruct the graph with high precision (AUC) of 0.77 (0.65), 0.72 (0.65) and 0.95 (0.94) for Citeseer, Cora and Pubmed respectively.
%A serious implication of reconstruction is link inference, where the adversary can infer the presence of an edge between two nodes in a graph with high accuracy of 93.39\% (Cora), 90.73\% (Citeseer) and 57.28\% (Pubmed) compared to 50\% random guess.
%Finally, we show that graph embeddings are strongly correlated to sensitive node attributes from which the adversary can infer gender and location for LastFM data (F1: 0.65 for DeepWalk and 0.83 for Node2Vec) and Facebook (F1: 0.59 for DeepWalk and 0.61 for Node2Vec).


%We further extend the attack to whitebox setting where adversary has access to the released node embeddings to indicate an adversary advantage as high as 72\%.
%These attacks exploit the fact graph embedding algorithms leave a distinguishable footprint between train and test data records.
%We then propose Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings.
%In this threat model, we show the adversary can reconstruct the graph with high precision (AUC) of 0.77 (0.65), 0.72 (0.65) and 0.95 (0.94) for Citeseer, Cora and Pubmed respectively.
%A serious implication of reconstruction is link inference, where the adversary can infer the presence of an edge between two nodes in a graph with high accuracy of 93.39\% (Cora), 90.73\% (Citeseer) and 57.28\% (Pubmed) compared to 50\% random guess.
%Finally, we show that graph embeddings are strongly correlated to sensitive node attributes from which the adversary can infer location from LastFM data (F1: 0.65 for DeepWalk and 0.83 for Node2Vec) and gender from Facebook data (F1: 0.59 for DeepWalk and 0.61 for Node2Vec).

\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002978.10003029.10011150</concept_id>
<concept_desc>Security and privacy~Privacy protections</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Privacy protections}
\ccsdesc[500]{Computing methodologies~Machine learning}




\keywords{Privacy Leakage, Inference Attacks, Graph Neural Networks, Graph Embeddings.}


\maketitle

\input{introduction}
\input{background}
\input{attacks}
\input{setting}
\input{evaluation}
%\input{mitigation}
\input{related}
\input{conclusions}

%{\footnotesize
%\bibliographystyle{IEEEtranS}
%\bibliography{paper.bib}
%}


\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}


\end{document}
\endinput
