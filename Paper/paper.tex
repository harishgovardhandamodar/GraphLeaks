%\documentclass[conference]{IEEEtran}
\documentclass[sigconf]{acmart}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\usepackage{multirow}
\usepackage{dblfloatfix}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{backgrounds, positioning, fit}
\usetikzlibrary{shapes.geometric}
\usepackage{amsmath}
\usetikzlibrary{patterns}
\usetikzlibrary{pgfplots.groupplots}
\usepackage{subfigure}
\usepackage{float}
\usepackage{wrapfig}


\begin{document}
%\pagestyle{plain}

\title{Quantifying Privacy Leakage in Graph Embedding}


%\author{
%    \IEEEauthorblockN{Vasisht Duddu\IEEEauthorrefmark{1}, Antoine Boutet\IEEEauthorrefmark{1},  Virat Shejwalkar\IEEEauthorrefmark{2}}
%    \IEEEauthorblockA{\IEEEauthorrefmark{1}Univ Lyon, INSA Lyon, Inria, CITI}
%    \IEEEauthorblockA{\IEEEauthorrefmark{2}University of Massachusetts Amherst}
%    \IEEEauthorblockA{vduddu@tutamail.com, antoine.boutet@insa-lyon.fr, vshejwalkar@cs.umass.edu}
%}




%\maketitle

\begin{abstract}


Graph embeddings have been proposed to map graph data to low dimensional space for downstream processing such as node classification and link prediction. With the increasing collection of personal data, graph embeddings can be trained on private and sensitive data. In this work, we quantify the privacy leakage in graph embeddings through three inference attacks targeting Graph Neural Networks. We propose a membership inference attack to infer whether a graph node corresponding to individual user's data was member of the model's training or not. We consider a blackbox setting where the adversary exploits the output prediction scores, and a whitebox setting where the adversary has also access to the released node embeddings. This attack provides an accuracy up to 28\% (blackbox) 36\% (whitebox) beyond random guess by exploiting the distinguishable footprint between train and test data records left by the graph embedding. We propose a Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings. Here, the adversary can reconstruct the graph with more than 80\% of accuracy and link inference between two nodes around 30\% more confidence than a random guess. We then propose an attribute inference attack where the adversary aims to infer a sensitive attribute. We show that graph embeddings are strongly correlated to node attributes from which the adversary can infer sensitive information such as gender and location.









%Graphs are ubiquitous to model the relationship between nodes representing various types of data.
%Consequently, graph embedding algorithms have been proposed to map graph data to low dimensional space for downstream processing such as node classification and link prediction.
%With the increasing collection and processing of personal data, graph embeddings can be trained on potentially private and sensitive graph data.
%In this work, we study and quantify the privacy leakage in graph embeddings by proposing novel inference attacks specific to Graph based deep learning models.
%Given a blackbox Graph Neural Network for node classification, we propose membership inference attack to infer whether a graph node corresponding to individual user's data was member of the model's training or not by exploiting the output prediction scores.
%In this blackbox setting, we show that the attack gives the adversary an inference advantage as high as 56\% beyond random guess.
%We further extend the attack to whitebox setting where adversary has access to the released node embeddings to indicate an adversary advantage as high as 72\%.
%These attacks exploit the fact graph embedding algorithms leave a distinguishable footprint between train and test data records.
%We then propose Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings.
%In this threat model, we show the adversary can reconstruct the graph with high precision (AUC) of 0.77 (0.65), 0.72 (0.65) and 0.95 (0.94) for Citeseer, Cora and Pubmed respectively.
%A serious implication of reconstruction is link inference, where the adversary can infer the presence of an edge between two nodes in a graph with high accuracy of 93.39\% (Cora), 90.73\% (Citeseer) and 57.28\% (Pubmed) compared to 50\% random guess.
%Finally, we show that graph embeddings are strongly correlated to sensitive node attributes from which the adversary can infer gender and location for LastFM data (F1: 0.65 for DeepWalk and 0.83 for Node2Vec) and Facebook (F1: 0.59 for DeepWalk and 0.61 for Node2Vec).


%We further extend the attack to whitebox setting where adversary has access to the released node embeddings to indicate an adversary advantage as high as 72\%.
%These attacks exploit the fact graph embedding algorithms leave a distinguishable footprint between train and test data records.
%We then propose Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings.
%In this threat model, we show the adversary can reconstruct the graph with high precision (AUC) of 0.77 (0.65), 0.72 (0.65) and 0.95 (0.94) for Citeseer, Cora and Pubmed respectively.
%A serious implication of reconstruction is link inference, where the adversary can infer the presence of an edge between two nodes in a graph with high accuracy of 93.39\% (Cora), 90.73\% (Citeseer) and 57.28\% (Pubmed) compared to 50\% random guess.
%Finally, we show that graph embeddings are strongly correlated to sensitive node attributes from which the adversary can infer location from LastFM data (F1: 0.65 for DeepWalk and 0.83 for Node2Vec) and gender from Facebook data (F1: 0.59 for DeepWalk and 0.61 for Node2Vec).

\end{abstract}
\keywords{Privacy Leakage, Inference Attacks, Graph Neural Networks, Graph Embeddings.}
%\begin{IEEEkeywords}
%Privacy Leakage, Inference Attacks, Graph Neural Networks, Graph Embeddings.
%\end{IEEEkeywords}

\maketitle

\input{introduction}
\input{background}
\input{attacks}
\input{setting}
\input{evaluation}
%\input{mitigation}
\input{related}
\input{conclusions}

%{\footnotesize
%\bibliographystyle{IEEEtranS}
%\bibliography{paper.bib}
%}

\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}


\end{document}
\endinput
