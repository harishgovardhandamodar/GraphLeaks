\documentclass[conference]{IEEEtran}


\usepackage{multirow}
\usepackage{dblfloatfix}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{backgrounds, positioning, fit}
\usetikzlibrary{shapes.geometric}
\usepackage{amsmath}
\usetikzlibrary{patterns}
\usetikzlibrary{pgfplots.groupplots}
\usepackage{subfigure}
\usepackage{float}
\usepackage{wrapfig}


\begin{document}
%\pagestyle{plain}

\title{Privacy Leakage in Graph Embedding}


%\author{
%    \IEEEauthorblockN{Vasisht Duddu\IEEEauthorrefmark{1}, Antoine Boutet\IEEEauthorrefmark{1},  Virat Shejwalkar\IEEEauthorrefmark{2}}
%    \IEEEauthorblockA{\IEEEauthorrefmark{1}Univ Lyon, INSA Lyon, Inria, CITI}
%    \IEEEauthorblockA{\IEEEauthorrefmark{2}University of Massachusetts Amherst}
%    \IEEEauthorblockA{vduddu@tutamail.com, antoine.boutet@insa-lyon.fr, vshejwalkar@cs.umass.edu}
%}




\maketitle

\begin{abstract}
Graphs are ubiquitous to model the relationship between nodes representing various types of data.
Consequently, graph embedding algorithms have been proposed to map graph data to low dimensional space for downstream processing such as node classification and link prediction.
With the increasing collection and processing of personal data, graph embeddings can be trained on potentially private and sensitive graph data.
In this work, we study and quantify the privacy leakage in graph embeddings by proposing novel inference attacks specific to Graph based deep learning models.
Given a blackbox Graph Neural Network for node classification, we propose membership inference attack to infer whether a graph node corresponding to individual user's data was member of the model's training or not by exploiting the output prediction scores.
In this blackbox setting, we show that the attack gives the adversary an inference advantage as high as 56\% beyond random guess.
We further extend the attack to whitebox setting where adversary has access to the released node embeddings to indicate an adversary advantage as high as 72\%.
These attacks exploit the fact graph embedding algorithms leave a distinguishable footprint between train and test data records.
We then propose Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings.
In this threat model, we show the adversary can reconstruct the graph with high precision (AUC) of 0.77 (0.65), 0.72 (0.65) and 0.95 (0.94) for Citeseer, Cora and Pubmed respectively.
A serious implication of reconstruction is link inference, where the adversary can infer the presence of an edge between two nodes in a graph with high accuracy of 93.39\% (Cora), 90.73\% (Citeseer) and 57.28\% (Pubmed) compared to 50\% random guess.
Finally, we show that graph embeddings are strongly correlated to sensitive node attributes from which the adversary can infer gender and location for LastFM data (F1: 0.65 for DeepWalk and 0.83 for Node2Vec) and Facebook (F1: 0.59 for DeepWalk and 0.61 for Node2Vec).
\end{abstract}

\begin{IEEEkeywords}
Privacy Leakage, Inference Attacks, Graph Neural Networks, Graph Embeddings.
\end{IEEEkeywords}

\input{introduction}
\input{background}
\input{attacks}
\input{setting}
\input{evaluation}
%\input{mitigation}
\input{related}
\input{conclusions}

{\footnotesize
\bibliographystyle{IEEEtranS}
\bibliography{paper.bib}
}



\end{document}
\endinput
